import argparse
import asyncio
import glob
import json
import os
from pathlib import Path
from typing import List

from src.multi_threaded_extractor import extract_documents_async, extract_problems_async, ExtractionConfig

class FileExtractor:
    """CPGraph æ–‡ä»¶æå–å™¨"""

    def __init__(self):
        self.supported_extensions = ['.md', '.txt', '.html', '.json', '.jsonl']
        self.encodings = ['utf-8', 'gbk']

    def extract(self, input_path: str) -> List[str]:
        """æå–æ–‡ä»¶"""
        path = Path(input_path)

        if not path.exists():
            raise FileNotFoundError(f"è·¯å¾„ä¸å­˜åœ¨: {input_path}")

        if path.is_dir():
            texts = self._extract_from_folder(path)
        elif path.suffix == '.jsonl':
            texts = self._extract_from_jsonl(path)
        elif path.suffix == '.json':
            texts = self._extract_from_json(path)
        else:
            texts = self._extract_from_text_file(path)

        return texts

    def _extract_from_folder(self, folder_path: Path) -> List[str]:
        """ä»æ–‡ä»¶å¤¹æå–"""
        texts = []
        pattern = str(folder_path / "**" / "*")

        for file_path in glob.glob(pattern, recursive=True):
            path = Path(file_path)
            if path.is_file() and self._is_supported(path):
                content = self._read_file(path)
                if content and content.strip():
                    texts.append(content)

        return texts

    def _extract_from_jsonl(self, jsonl_path: Path) -> List[str]:
        """ä»JSONLæå– - æ¯è¡ŒJSONç›´æ¥dumps"""
        texts = []

        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        data = json.loads(line)
                        texts.append(json.dumps(data, ensure_ascii=False))
                    except json.JSONDecodeError:
                        continue

        return texts

    def _extract_from_json(self, json_path: Path) -> List[str]:
        """ä»JSONæ–‡ä»¶æå–"""
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return [json.dumps(data, ensure_ascii=False)]

    def _extract_from_text_file(self, file_path: Path) -> List[str]:
        """ä»æ–‡æœ¬æ–‡ä»¶æå–"""
        content = self._read_file(file_path)
        return [content] if content and content.strip() else []

    def _is_supported(self, path: Path) -> bool:
        return path.suffix in self.supported_extensions

    def _read_file(self, file_path: Path) -> str:
        for encoding in self.encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    return f.read()
            except UnicodeDecodeError:
                continue
        return ""
async def create_llm_func():
    async def llm_model_func(
        prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
    ) -> str:
        from lightrag.llm.openai import openai_complete_if_cache
        return await openai_complete_if_cache(
            os.getenv("LLM_MODEL", "deepseek-chat"),
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=os.getenv("LLM_BINDING_API_KEY"),
            base_url=os.getenv("LLM_BINDING_HOST", "https://api.deepseek.com"),
            **kwargs,
        )
    return llm_model_func

async def extract_and_extract(input_path: str, mode: str):
    print(f"ğŸ” æå–æ–‡ä»¶: {input_path}")
    extractor = FileExtractor()
    texts = extractor.extract(input_path)
    print(f"âœ… æå–å®Œæˆ: {len(texts)} ä¸ªæ–‡æ¡£")

    print("ğŸ¤– åˆå§‹åŒ–LLMå‡½æ•°...")
    llm_func = await create_llm_func()

    if mode == "normal":
        config = ExtractionConfig(
            thread_count=int(os.getenv("THREAD_COUNT", "16")),
            max_concurrent_per_thread=int(os.getenv("MAX_CONCURRENT", "8")),
            chunk_token_size=int(os.getenv("CHUNK_SIZE", "1200")),
            chunk_overlap_token_size=int(os.getenv("CHUNK_OVERLAP_SIZE", "100")),
            output_dir=os.getenv("EXTRACTOR_OUTPUT_DIR", "./kg_storage"),
            enable_progress_logging=True,
            log_interval=5
        )
        print(f"ğŸ“ æ¨¡å¼: æ™®é€šæ–‡æœ¬æå–")

    elif mode == "problem":
        config = ExtractionConfig(
            thread_count=int(os.getenv("THREAD_COUNT", "16")),
            max_concurrent_per_thread=int(os.getenv("MAX_CONCURRENT", "8")),
            chunk_token_size=10000000000,
            chunk_overlap_token_size=int(os.getenv("CHUNK_OVERLAP_SIZE", "100")),
            extraction_mode="problem",
            output_dir=os.getenv("EXTRACTOR_OUTPUT_DIR", "./kg_storage"),
            enable_progress_logging=True,
            log_interval=5
        )
        print(f"ğŸ“ æ¨¡å¼: é¢˜ç›®æ–‡æœ¬æå–")

    print("ğŸš€ å¼€å§‹å®ä½“æå–...")

    if mode == "normal":
        entities, relations = await extract_documents_async(
            documents=texts,
            llm_func=llm_func,
            config=config
        )
    else:
        entities, relations = await extract_problems_async(
            documents=texts[:10],
            llm_func=llm_func,
            config=config
        )

    print(f"\nâœ… å®ä½“æå–å®Œæˆ!")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - æ–‡æ¡£æ•°é‡: {len(texts)}")
    print(f"   - æå–å®ä½“: {len(entities)}")
    print(f"   - æå–å…³ç³»: {len(relations)}")
    print(f"   - è¾“å‡ºç›®å½•: {config.output_dir}")

    if mode == "problem":
        problem_count = sum(1 for e in entities if "é¢˜ç›®" in e.get("entity_type_dim2", ""))
        solution_count = sum(1 for e in entities if "é¢˜è§£" in e.get("entity_type_dim2", ""))
        trick_count = sum(1 for e in entities if "æŠ€å·§" in e.get("entity_type_dim2", ""))
        kp_count = sum(1 for e in entities if "æ¦‚å¿µ" in e.get("entity_type_dim2", ""))

        print(f"   - é¢˜ç›®å®ä½“: {problem_count}")
        print(f"   - è§£æ³•å®ä½“: {solution_count}")
        print(f"   - æŠ€å·§å®ä½“: {trick_count}")
        print(f"   - çŸ¥è¯†ç‚¹å®ä½“: {kp_count}")

async def main():
    parser = argparse.ArgumentParser(
        description="CPGraph æ–‡ä»¶æå–+å®ä½“æå–å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python file_extract.py ./documents --mode normal
  python file_extract.py ./problems.jsonl --mode problem

æå–æ¨¡å¼:
  normal   - æ™®é€šæ–‡æœ¬å®ä½“æå–
  problem  - é¢˜ç›®ä¸“ç”¨å®ä½“æå–ï¼ˆä¸åˆ†å‰²é¢˜ç›®å’Œé¢˜è§£ï¼‰
        """
    )

    parser.add_argument('input_path', help='è¾“å…¥è·¯å¾„ï¼ˆæ–‡ä»¶å¤¹æˆ–æ–‡ä»¶ï¼‰')
    parser.add_argument('--mode', choices=['normal', 'problem'], default='normal',
                       help='æå–æ¨¡å¼: normal(æ™®é€šæ–‡æœ¬) æˆ– problem(é¢˜ç›®æ–‡æœ¬)ï¼Œé»˜è®¤ normal')

    args = parser.parse_args()

    try:
        await extract_and_extract(args.input_path, args.mode)
        return 0
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    from src.llm_tracker import cleanup

    try:
        exit_code = asyncio.run(main())
        exit(exit_code)
    finally:
        cleanup()