"""
MCP Server for Neo4j Knowledge Graph Query Engine

åŸºäº FastMCP çš„çŸ¥è¯†å›¾è°±æŸ¥è¯¢æœåŠ¡ï¼Œä½¿ç”¨ Neo4j æ•°æ®åº“å’Œ Cypher æŸ¥è¯¢ã€‚
å‚è€ƒ https://gofastmcp.com äº†è§£æ›´å¤šä¿¡æ¯ã€‚
"""

import asyncio
import os
import json
import logging
from typing import List, Dict, Any, Optional
from fastmcp import FastMCP
from src.kg_query_engine import KGQueryEngine

# Neo4j ç›¸å…³å¯¼å…¥ - ä½¿ç”¨å®˜æ–¹å¼‚æ­¥é©±åŠ¨
from neo4j import AsyncGraphDatabase
from dotenv import load_dotenv
import pathlib
from lightrag.utils import setup_logger, get_env_value

# è®¾ç½®æ—¥å¿— - å¤ç”¨LightRAG logger
log_level = get_env_value("LOG_LEVEL", "INFO", str).upper()
# æ˜¯å¦å¯ç”¨æ–‡ä»¶æ—¥å¿—ï¼ˆé»˜è®¤ä¸ºtrueï¼‰
enable_file_logging = get_env_value("LOG_FILE_ENABLE", "true", str).lower() == "true"
# æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨ LOG_DIR + cprag.logï¼‰
log_file_path = None
if enable_file_logging:
    log_dir = get_env_value("LOG_DIR", ".", str)
    log_file_path = os.path.abspath(os.path.join(log_dir, "cprag.log"))

setup_logger(
    logger_name="CPGraph",
    level=log_level,
    add_filter=False,
    log_file_path=log_file_path,
    enable_file_logging=enable_file_logging
)
logger = logging.getLogger("CPGraph")

# åŠ è½½ .env æ–‡ä»¶ - å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
script_dir = pathlib.Path(__file__).parent.resolve()
project_root = script_dir.parent

for env_path in [project_root / ".env", script_dir / ".env", pathlib.Path(".env")]:
    if env_path.exists():
        load_dotenv(dotenv_path=str(env_path), override=False)
        logger.info(f"[OK] Load environment variables: {env_path}")
        break

def filter_embedding_fields(data):
    """
    è¿‡æ»¤æ‰ embedding ç›¸å…³å­—æ®µï¼Œé˜²æ­¢ token æµªè´¹å’Œä¸Šä¸‹æ–‡è¶…é™

    Args:
        data: å•ä¸ªæ•°æ®å­—å…¸æˆ–æ•°æ®å­—å…¸åˆ—è¡¨

    Returns:
        è¿‡æ»¤åçš„æ•°æ®
    """
    # å®šä¹‰éœ€è¦è¿‡æ»¤çš„å­—æ®µåæ¨¡å¼
    embedding_field_patterns = [
        'embedding',
        'vector',
        'embedding_vector',
        'emb',
        'vec',
        'embedding_array',
        'embedding_list',
    ]

    def filter_single_item(item):
        """è¿‡æ»¤å•ä¸ªæ•°æ®é¡¹"""
        if not isinstance(item, dict):
            return item

        filtered_item = {}
        for key, value in item.items():
            # æ£€æŸ¥å­—æ®µåæ˜¯å¦åŒ¹é… embedding æ¨¡å¼
            is_embedding_field = any(
                pattern.lower() in key.lower() for pattern in embedding_field_patterns
            )

            if not is_embedding_field:
                filtered_item[key] = value
            else:
                logger.debug(f"Filtered out embedding field: {key}")

        return filtered_item

    # å¤„ç†å•ä¸ªæ•°æ®é¡¹æˆ–åˆ—è¡¨
    if isinstance(data, list):
        return [filter_single_item(item) for item in data]
    else:
        return filter_single_item(data)


def safe_execute_cypher_with_filter(query_engine, *args, **kwargs):
    """
    å®‰å…¨æ‰§è¡Œ Cypher æŸ¥è¯¢ï¼Œè‡ªåŠ¨è¿‡æ»¤ embedding å­—æ®µ

    Args:
        query_engine: KGQueryEngine å®ä¾‹
        *args: ä¼ é€’ç»™ execute_cypher çš„ä½ç½®å‚æ•°
        **kwargs: ä¼ é€’ç»™ execute_cypher çš„å…³é”®å­—å‚æ•°

    Returns:
        è¿‡æ»¤åçš„æŸ¥è¯¢ç»“æœ
    """
    import asyncio

    # è·å–äº‹ä»¶å¾ªç¯
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

    # æ‰§è¡ŒæŸ¥è¯¢
    results = loop.run_until_complete(
        query_engine.execute_cypher(*args, **kwargs)
    )

    # è¿‡æ»¤ç»“æœ
    filtered_results = filter_embedding_fields(results)

    # è®°å½•è¿‡æ»¤ç»Ÿè®¡
    if isinstance(results, list) and len(results) > 0:
        original_size = len(str(results))
        filtered_size = len(str(filtered_results))
        saved_ratio = (original_size - filtered_size) / original_size * 100
        logger.info(f"Filtered embedding fields: saved {saved_ratio:.1f}% of response size")

    return filtered_results



# Initialize FastMCP server
mcp = FastMCP(
    "OI Knowledge Graph Query Server",
    instructions="LightRAG çŸ¥è¯†å›¾è°±æŸ¥è¯¢æœåŠ¡ï¼Œä¸“é—¨ç”¨äºä¿¡æ¯å­¦ç«èµ›çŸ¥è¯†æŸ¥è¯¢"
)

class KGQueryService:
    """Neo4j çŸ¥è¯†å›¾è°±æŸ¥è¯¢æœåŠ¡æ ¸å¿ƒç±» - ä½¿ç”¨å®˜æ–¹å¼‚æ­¥é©±åŠ¨"""

    def __init__(self):
        self.driver = None  # AsyncDriver å®ä¾‹ï¼ˆå†…ç½®è¿æ¥æ± ï¼‰
        self.kg_engine = None  # å•ä¸ªå¼•æ“å®ä¾‹
        self.initialized = False
        self.chunks_cache: Optional[Dict] = None  # æ–°å¢ï¼šchunks ç¼“å­˜
        self.chunks_cache_path: Optional[str] = None  # æ–°å¢ï¼šç¼“å­˜æ–‡ä»¶è·¯å¾„
        self.chunks_cache_loaded: bool = False  # æ–°å¢ï¼šç¼“å­˜åŠ è½½æ ‡å¿—
        logger.info(f"[OK] KGQueryService initialized")

    async def initialize(
        self,
        neo4j_uri: Optional[str] = None,
        neo4j_user: Optional[str] = None,
        neo4j_password: Optional[str] = None,
        default_limit: int = 30,
        embedding_func=None,
    ):
        """
        åˆå§‹åŒ– Neo4j çŸ¥è¯†å›¾è°±æŸ¥è¯¢å¼•æ“

        Args:
            neo4j_uri: Neo4j è¿æ¥ URI
            neo4j_user: Neo4j ç”¨æˆ·å
            neo4j_password: Neo4j å¯†ç 
            default_limit: é»˜è®¤æŸ¥è¯¢é™åˆ¶ (max 30)
            embedding_func: åµŒå…¥å‡½æ•°ï¼Œç”¨äºå‘é‡æœç´¢
        """
        if self.initialized:
            logger.warning("[WARNING] KGQueryEngine already initialized, skip duplicate initialization")
            return

        try:
            # ä»ç¯å¢ƒå˜é‡è¯»å– Neo4j é…ç½®
            uri = neo4j_uri or os.getenv("NEO4J_URI", "bolt://localhost:7687")
            user = neo4j_user or os.getenv("NEO4J_USERNAME", "neo4j")
            password = neo4j_password or os.getenv("NEO4J_PASSWORD", "password")

            logger.info(f"[INFO] Initializing Neo4j AsyncDriver...")
            logger.info(f"  URI: {uri}")
            logger.info(f"  User: {user}")
            logger.info(f"  Default limit: {default_limit}")
            logger.info(f"  Vector search: {'[ENABLED]' if embedding_func else '[DISABLED]'}")

            # åˆ›å»º AsyncDriverï¼ˆå†…ç½®è¿æ¥æ± ç®¡ç†ï¼‰
            self.driver = AsyncGraphDatabase.driver(uri, auth=(user, password))

            # éªŒè¯è¿æ¥
            await self.driver.verify_connectivity()
            logger.info(f"[OK] Neo4j connection verified")

            # åˆ›å»ºå•ä¸ªå¼•æ“å®ä¾‹
            self.kg_engine = KGQueryEngine(
                driver=self.driver,
                default_limit=default_limit,
                embedding_func=embedding_func,
            )
            logger.info(f"[OK] KGQueryEngine initialized")

            # è¯»å– chunks JSON è·¯å¾„é…ç½®ï¼ˆä½¿ç”¨ MERGED_DATA_DIR ç¯å¢ƒå˜é‡ï¼‰
            merged_data_dir = os.getenv("MERGED_DATA_DIR", str(project_root / "merged_data"))
            chunks_json_path = os.path.join(merged_data_dir, "chunks_backup.json")
            logger.info(f"[CONFIG] Using MERGED_DATA_DIR from env: {merged_data_dir}")

            self.chunks_cache_path = str(chunks_json_path)

            # é¢„åŠ è½½ chunks åˆ°ç¼“å­˜
            if os.path.exists(self.chunks_cache_path):
                try:
                    with open(self.chunks_cache_path, 'r', encoding='utf-8') as f:
                        self.chunks_cache = json.load(f)
                    self.chunks_cache_loaded = True
                    logger.info(f"[OK] Pre-loaded {len(self.chunks_cache)} chunks into cache")
                except Exception as e:
                    logger.warning(f"[WARNING] Failed to preload chunks cache: {e}")
                    self.chunks_cache = {}
                    self.chunks_cache_loaded = False
            else:
                logger.info(f"[INFO] Chunks file not found: {self.chunks_cache_path}, will use dynamic loading")
                self.chunks_cache = {}
                self.chunks_cache_loaded = False

            self.initialized = True
            logger.info(f"[OK] KGQueryService initialized successfully (async driver with built-in connection pool)")

        except Exception as e:
            logger.error(f"[ERROR] KGQueryEngine initialization failed: {e}")
            raise

    async def cleanup(self):
        """æ¸…ç†èµ„æºï¼Œå…³é—­é©±åŠ¨è¿æ¥"""
        if self.driver:
            await self.driver.close()
            logger.info("[OK] Neo4j driver closed")

    async def find_similar_entities(
        self,
        entity_query: str,
        top_k: int = 5,
        rerank: str = "pass",
    ) -> List[Dict[str, Any]]:
        """
        æ ¹æ®æŸ¥è¯¢å­—ç¬¦ä¸²æŸ¥æ‰¾ç›¸ä¼¼çš„å®ä½“

        Args:
            entity_query: å®ä½“æŸ¥è¯¢å­—ç¬¦ä¸²ï¼ˆåç§°æˆ–æè¿°ï¼‰
            top_k: è¿”å›çš„æœ€ç›¸ä¼¼å®ä½“æ•°é‡
            rerank: é‡æ’åºç­–ç•¥ï¼š
                - "pass": è·³è¿‡é‡æ’ï¼Œä½¿ç”¨åŸå§‹å‘é‡ç›¸ä¼¼åº¦æ’åºï¼ˆé»˜è®¤ï¼‰
                - "degree": åŸºäºå­å›¾åº¦æ•°é‡æ’åº

        Returns:
            ç›¸ä¼¼å®ä½“åˆ—è¡¨ï¼Œæ¯ä¸ªå®ä½“åŒ…å«åç§°ã€æè¿°ã€ç±»å‹ç­‰ä¿¡æ¯ã€‚
            å½“rerank="degree"æ—¶ï¼Œè¿˜ä¼šåŒ…å«degreeï¼ˆåº¦æ•°ï¼‰å’Œsimilarity_scoreï¼ˆç›¸ä¼¼åº¦åˆ†æ•°ï¼‰ç”¨äºè°ƒè¯•
        """
        if not self.initialized:
            logger.info("æœåŠ¡åˆå§‹åŒ–")
            await self.initialize()

        # è®¡ç®—å®é™…å‘é‡æ£€ç´¢çš„top_k
        vector_top_k = top_k * 4 if rerank == "degree" else top_k
        logger.info(f"[INFO] Finding similar entities: '{entity_query}' (top_k={top_k}, rerank={rerank}, vector_top_k={vector_top_k})")

        try:
            results = await self.kg_engine.find_similar_entities(
                entity_query=entity_query,
                top_k=vector_top_k,
            )

            # å¦‚æœéœ€è¦é‡æ’åº
            if rerank == "degree" and len(results) > top_k:
                results = await self._rerank_by_degree(results, top_k)
            elif rerank == "degree":
                # å³ä½¿æ²¡æœ‰æˆªæ–­ï¼Œä¹Ÿè®¡ç®—åº¦æ•°ç”¨äºè¿”å›
                results_with_scores = await self._rerank_by_degree(results, len(results))
                # æ¢å¤åŸå§‹é¡ºåºä½†ä¿ç•™åˆ†æ•°
                for i, r in enumerate(results):
                    if i < len(results_with_scores):
                        r["degree"] = results_with_scores[i].get("degree", 0)
                        # ä¿ç•™åŸå§‹çš„similarity_scoreï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œæˆ–è€…ä½¿ç”¨é‡æ’åºåçš„å€¼
                        r["similarity_score"] = results_with_scores[i].get("similarity_score", r.get("similarity_score", 0))
            elif rerank == "pagerank" and len(results) > top_k:
                results = await self._rerank_by_pagerank(results, top_k)
            elif rerank == "pagerank":
                # å³ä½¿æ²¡æœ‰æˆªæ–­ï¼Œä¹Ÿè®¡ç®— PageRank ç”¨äºè¿”å›
                results_with_scores = await self._rerank_by_pagerank(results, len(results))
                # æ¢å¤åŸå§‹é¡ºåºä½†ä¿ç•™åˆ†æ•°
                for i, r in enumerate(results):
                    if i < len(results_with_scores):
                        r["pagerank"] = results_with_scores[i].get("pagerank", 0)
                        # ä¿ç•™åŸå§‹çš„similarity_scoreï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œæˆ–è€…ä½¿ç”¨é‡æ’åºåçš„å€¼
                        r["similarity_score"] = results_with_scores[i].get("similarity_score", r.get("similarity_score", 0))

            # æ³¨æ„ï¼šç»“æœå·²ç»åœ¨ kg_query_engine.execute_cypher ä¸­è‡ªåŠ¨è¿‡æ»¤äº† embedding å­—æ®µ
            logger.info(f"[OK] Found {len(results)} similar entities (embedding fields automatically filtered)")

            return results

        except Exception as e:
            logger.error(f"[ERROR] Failed to find similar entities: {e}")
            raise

    async def _rerank_by_degree(
        self,
        candidates: List[Dict],
        final_k: int,
    ) -> List[Dict]:
        """
        åŸºäºå­å›¾åº¦æ•°é‡æ’åº

        Args:
            candidates: å€™é€‰å®ä½“åˆ—è¡¨
            final_k: æœ€ç»ˆè¿”å›çš„æ•°é‡

        Returns:
            é‡æ’åºåçš„ç»“æœï¼ŒåŒ…å«degreeå’Œsimilarity_scoreå­—æ®µç”¨äºè°ƒè¯•
        """
        if not candidates:
            return candidates

        # æå–å€™é€‰èŠ‚ç‚¹IDåˆ—è¡¨
        candidate_ids = [c["entity_name"] for c in candidates]

        # æ„å»ºå­å›¾å¹¶è®¡ç®—åº¦æ•°
        query = """
        UNWIND $candidate_ids as target_id
        MATCH (n {entity_id: target_id})
        WITH target_id, n

        MATCH (n)-[r]-(neighbor)
        WHERE neighbor.entity_id IN $candidate_ids
        WITH target_id, count(DISTINCT neighbor) as degree

        RETURN target_id, degree
        ORDER BY degree DESC
        """

        degree_results = await self.kg_engine.execute_cypher(
            query=query,
            parameters={"candidate_ids": candidate_ids}
        )

        # åˆ›å»ºåº¦æ•°æ˜ å°„
        degree_map = {r["target_id"]: r["degree"] for r in degree_results}

        # é‡æ’åºï¼šåº¦æ•°ä¸»å¯¼ï¼Œç›¸ä¼¼åº¦è¾…åŠ©
        # å…ˆä¸ºæ¯ä¸ªå€™é€‰è®¡ç®—rerank_scoreå¹¶æ’åºï¼Œä¿ç•™åº¦æ•°å’Œç›¸ä¼¼åº¦åˆ†æ•°
        reranked_with_scores = []
        for candidate in candidates:
            entity_name = candidate["entity_name"]
            degree = degree_map.get(entity_name, 0)
            # æ³¨æ„ï¼švector_searchè¿”å›çš„æ˜¯similarity_scoreå­—æ®µï¼Œä¸æ˜¯scoreå­—æ®µ
            similarity_score = candidate.get("similarity_score", 0) if "similarity_score" in candidate else 0
            rerank_score = degree * 1000 + similarity_score

            # ä¸ºå€™é€‰æ·»åŠ åº¦æ•°å’Œç›¸ä¼¼åº¦åˆ†æ•°å­—æ®µ
            candidate_with_scores = candidate.copy()
            candidate_with_scores["degree"] = degree
            candidate_with_scores["similarity_score"] = similarity_score

            reranked_with_scores.append((candidate_with_scores, rerank_score))

        # æŒ‰rerank_scoreæ’åº
        reranked_with_scores.sort(key=lambda x: x[1], reverse=True)

        # è¿”å›æœ€ç»ˆç»“æœï¼ˆåŒ…å«åº¦æ•°å’Œç›¸ä¼¼åº¦åˆ†æ•°å­—æ®µï¼‰
        final_results = [candidate for candidate, score in reranked_with_scores[:final_k]]

        # è¾“å‡ºè°ƒè¯•ä¿¡æ¯åˆ°æ—¥å¿—
        logger.info(f"[DEBUG] Reranked {len(candidates)} candidates, top 3 scores:")
        for i, (candidate, score) in enumerate(reranked_with_scores[:3]):
            logger.info(f"  {i+1}. {candidate['entity_name']}: degree={candidate['degree']}, similarity={candidate['similarity_score']:.4f}, rerank_score={score:.4f}")

        return final_results

    async def _rerank_by_pagerank(
        self,
        candidates: List[Dict],
        final_k: int,
    ) -> List[Dict]:
        """
        åŸºäºå…¨å›¾ PageRank é‡æ’åº

        ä½¿ç”¨ Neo4j çš„ PageRank ç®—æ³•è®¡ç®—æ¯ä¸ªå€™é€‰å®ä½“åœ¨å…¨å±€å›¾ä¸­çš„é‡è¦æ€§ï¼Œ
        ç„¶åç»“åˆåŸå§‹ç›¸ä¼¼åº¦åˆ†æ•°è¿›è¡Œé‡æ’åºã€‚

        Args:
            candidates: å€™é€‰å®ä½“åˆ—è¡¨
            final_k: æœ€ç»ˆè¿”å›çš„æ•°é‡

        Returns:
            é‡æ’åºåçš„ç»“æœï¼ŒåŒ…å« pagerank å’Œ similarity_score å­—æ®µ
        """
        if not candidates:
            return candidates

        # æå–å€™é€‰èŠ‚ç‚¹IDåˆ—è¡¨
        candidate_ids = [c["entity_name"] for c in candidates]

        try:
            # ç›´æ¥æŸ¥è¯¢å€™é€‰å®ä½“çš„ PageRank å€¼ï¼ˆå·²é¢„å…ˆè®¡ç®—å¹¶å­˜å‚¨åœ¨èŠ‚ç‚¹å±æ€§ä¸­ï¼‰
            query = """
            MATCH (e)
            WHERE e.entity_id IN $candidate_ids
            RETURN e.entity_id as entity_id, e.pagerank as pagerank
            ORDER BY e.pagerank DESC
            """

            pagerank_results = await self.kg_engine.execute_cypher(
                query=query,
                parameters={"candidate_ids": candidate_ids}
            )

            # åˆ›å»º PageRank æ˜ å°„
            pagerank_map = {r["entity_id"]: r["pagerank"] for r in pagerank_results}

            # é‡æ’åºï¼šPageRank ä¸»å¯¼ï¼Œç›¸ä¼¼åº¦è¾…åŠ©
            reranked_with_scores = []
            for candidate in candidates:
                entity_name = candidate["entity_name"]
                pagerank = pagerank_map.get(entity_name, 0)
                similarity_score = candidate.get("similarity_score", 0) if "similarity_score" in candidate else 0
                rerank_score = pagerank * 1000 + similarity_score

                # ä¸ºå€™é€‰æ·»åŠ  PageRank å’Œç›¸ä¼¼åº¦åˆ†æ•°å­—æ®µ
                candidate_with_scores = candidate.copy()
                candidate_with_scores["pagerank"] = pagerank
                candidate_with_scores["similarity_score"] = similarity_score

                reranked_with_scores.append((candidate_with_scores, rerank_score))

            # æŒ‰ rerank_score æ’åº
            reranked_with_scores.sort(key=lambda x: x[1], reverse=True)

            # è¿”å›æœ€ç»ˆç»“æœï¼ˆåŒ…å« PageRank å’Œç›¸ä¼¼åº¦åˆ†æ•°å­—æ®µï¼‰
            final_results = [candidate for candidate, score in reranked_with_scores[:final_k]]

            # è¾“å‡ºè°ƒè¯•ä¿¡æ¯
            logger.info(f"[DEBUG] PageRank reranked {len(candidates)} candidates, top 3 scores:")
            for i, (candidate, score) in enumerate(reranked_with_scores[:3]):
                logger.info(f"  {i+1}. {candidate['entity_name']}: pagerank={candidate['pagerank']:.4f}, similarity={candidate['similarity_score']:.4f}, rerank_score={score:.4f}")

            return final_results

        except Exception as e:
            logger.warning(f"[WARNING] PageRank query failed: {e}, falling back to original order")
            # å¦‚æœ PageRank æŸ¥è¯¢å¤±è´¥ï¼Œè¿”å›åŸå§‹ç»“æœ
            return candidates[:final_k]

    async def execute_custom_cypher(
        self,
        query: str,
        parameters: Optional[Dict[str, Any]] = None,
        limit: Optional[int] = None,
        vector_params: Optional[Dict[str, bool]] = None,
    ) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œè‡ªå®šä¹‰ Cypher æŸ¥è¯¢

        Args:
            query: Cypher æŸ¥è¯¢å­—ç¬¦ä¸²
            parameters: æŸ¥è¯¢å‚æ•°
            limit: ç»“æœé™åˆ¶ (max 30)
            vector_params: å‘é‡å‚æ•°æ˜ å°„ï¼Œæ ‡è®°å“ªäº›å‚æ•°éœ€è¦è½¬æ¢ä¸ºå‘é‡

        Returns:
            æŸ¥è¯¢ç»“æœåˆ—è¡¨
        """
        if not self.initialized:
            await self.initialize()

        logger.info(f"[INFO] Executing custom Cypher query")
        logger.info(f"  Query length: {len(query)} characters")
        logger.info(f"  Parameters count: {len(parameters) if parameters else 0}")
        logger.info(f"  Vector parameters: {list(vector_params.keys()) if vector_params else 'none'}")

        try:
            results = await self.kg_engine.execute_cypher(
                query=query,
                parameters=parameters,
                limit=limit,
                vector_params=vector_params,
            )
            logger.info(f"[OK] Query successful, returned {len(results)} records")
            return results

        except Exception as e:
            logger.error(f"[ERROR] Custom Cypher query failed: {e}")
            raise

    async def get_entity_chunks(
        self,
        entity_id: str,
        include_content: bool = False,
    ) -> List[Dict[str, Any]]:
        """
        è·å–å®ä½“å…³è”çš„æ‰€æœ‰ chunk åˆ—è¡¨

        Args:
            entity_id: å®ä½“çš„ç²¾ç¡® IDï¼ˆåŒ…å«éšæœºåç¼€ï¼Œå¦‚ï¼šçº¿æ®µæ ‘<QyCKb7>ï¼‰
            include_content: æ˜¯å¦åŒ…å« chunk çš„å®Œæ•´å†…å®¹ï¼ˆé»˜è®¤ Falseï¼Œåªè¿”å›åŸºæœ¬ä¿¡æ¯ï¼‰

        Returns:
            å…³è”çš„ chunk åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« chunk_idã€contentã€tokensã€file_path ç­‰ä¿¡æ¯
        """
        if not self.initialized:
            await self.initialize()

        logger.info(f"[INFO] Getting chunks for entity: '{entity_id}' (include_content={include_content})")

        try:
            # æŸ¥è¯¢å®ä½“çš„ source_id å±æ€§ï¼ˆåŒ…å«å…³è”çš„ chunk IDsï¼‰
            cypher = """
            MATCH (e {entity_id: $entity_id})
            RETURN e.source_id as source_id
            LIMIT 1
            """

            results = await self.kg_engine.execute_cypher(
                query=cypher,
                parameters={"entity_id": entity_id},
                limit=1
            )

            if not results:
                logger.warning(f"Entity '{entity_id}' not found")
                return []

            source_id_str = results[0]["source_id"]
            if not source_id_str:
                logger.info(f"Entity '{entity_id}' has no associated chunks")
                return []

            # è§£æ chunk IDs - ä½¿ç”¨å¤šç§å¯èƒ½çš„åˆ†éš”æ–¹å¼

            chunk_ids = [chunk_id.strip() for chunk_id in source_id_str.split("<SEP>") if chunk_id.strip()]

            # å»é‡
            unique_chunk_ids = list(dict.fromkeys(chunk_ids))

            logger.info(f"Found {len(unique_chunk_ids)} unique chunks for entity '{entity_id}' (parsed from {len(chunk_ids)} items)")
            if len(chunk_ids) != len(unique_chunk_ids):
                logger.debug(f"Removed {len(chunk_ids) - len(unique_chunk_ids)} duplicates")

            if not include_content:
                # åªè¿”å› chunk ID åˆ—è¡¨ï¼ˆè½»é‡çº§ï¼‰
                return [
                    {
                        "chunk_id": chunk_id,
                        "status": "available"
                    }
                    for chunk_id in unique_chunk_ids
                ]

            # å¦‚æœéœ€è¦å®Œæ•´å†…å®¹ï¼Œä¼˜å…ˆä»ç¼“å­˜è·å–
            chunk_details = []
            chunks_map = {}

            if self.chunks_cache_loaded and self.chunks_cache:
                # ä½¿ç”¨é¢„åŠ è½½çš„ç¼“å­˜
                chunks_map = self.chunks_cache
                logger.debug(f"Using pre-loaded chunks cache ({len(chunks_map)} items)")
            else:
                # åŠ¨æ€åŠ è½½ï¼ˆfallbackï¼‰
                if os.path.exists(self.chunks_cache_path):
                    try:
                        with open(self.chunks_cache_path, 'r', encoding='utf-8') as f:
                            chunks_map = json.load(f)
                        logger.info(f"Dynamic loaded {len(chunks_map)} chunks from {self.chunks_cache_path}")
                    except Exception as e:
                        logger.warning(f"Failed to load chunks.json: {e}")
                else:
                    logger.info(f"Chunks file not found: {self.chunks_cache_path}")

            for chunk_id in unique_chunk_ids:
                chunk_detail = {
                    "chunk_id": chunk_id,
                    "status": "available"
                }

                # ä»æ˜ å°„ä¸­è·å– chunk æ•°æ®
                chunk_data = chunks_map.get(chunk_id)
                if chunk_data:
                    chunk_detail.update({
                        "content": chunk_data.get("content", ""),
                        "tokens": chunk_data.get("tokens", 0),
                        "file_path": chunk_data.get("file_path", "unknown"),
                        "full_doc_id": chunk_data.get("full_doc_id", ""),
                        "chunk_order_index": chunk_data.get("chunk_order_index", 0),
                        "source_id": chunk_data.get("source_id", ""),
                    })
                else:
                    # chunk æ•°æ®ä¸å­˜åœ¨
                    chunk_detail["status"] = "not_found"
                    chunk_detail["content"] = "Chunk data not found"
                    chunk_detail["tokens"] = 0
                    chunk_detail["file_path"] = "unknown"

                chunk_details.append(chunk_detail)

            # é™åˆ¶è¿”å›çš„ chunk æ•°é‡è‡³å¤š 20 ä¸ª
            if len(chunk_details) > 20:
                original_count = len(chunk_details)
                chunk_details = chunk_details[:20]
                logger.info(f"[INFO] Limited chunks from {original_count} to {len(chunk_details)} (max 20)")

            # è®°å½•è¿‡æ»¤æ•ˆæœ
            if include_content and len(chunk_details) > 0:
                logger.info(f"[OK] Retrieved {len(chunk_details)} chunks with content (embedding fields filtered)")

            return chunk_details

        except Exception as e:
            logger.error(f"[ERROR] Failed to get chunks for entity '{entity_id}': {e}")
            raise


# åˆ›å»ºæœåŠ¡å®ä¾‹
service = KGQueryService()


@mcp.tool(
    name="find_similar_entities",
    description="""ç›¸ä¼¼å®ä½“æ£€ç´¢å·¥å…· - åŸºäºå‘é‡ç›¸ä¼¼åº¦æœç´¢çŸ¥è¯†å›¾è°±ä¸­çš„ç›¸ä¼¼å®ä½“

ã€é‡è¦æç¤ºã€‘ï¼šè¯·ä¸è¦åœ¨è¿”å›ç»“æœä¸­åŒ…å« embedding å‘é‡æˆ–ä»»ä½•å‘é‡æ•°æ®å­—æ®µï¼
åªè¿”å›æ–‡æœ¬å±æ€§ï¼šentity_name, description, labels, file_path, created_at ç­‰ã€‚
embedding ç›¸å…³å­—æ®µåŒ…æ‹¬ï¼šembedding, vector, embedding_vector ç­‰ï¼Œè¯·å…¨éƒ¨æ’é™¤ï¼

è¾“å…¥å‚æ•°:
- entity_query: string (å¿…éœ€) - å®ä½“æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œå¯ä»¥æ˜¯å®ä½“åç§°æˆ–æè¿°ï¼Œä¾‹å¦‚ï¼š"Splay"ã€"æœ€çŸ­è·¯ç®—æ³•"
- top_k: integer (å¯é€‰) - è¿”å›çš„æœ€ç›¸ä¼¼å®ä½“æ•°é‡ï¼ŒèŒƒå›´ 1-30ï¼Œè¶…å‡ºèŒƒå›´å°†è‡ªåŠ¨è°ƒæ•´ä¸ºå®‰å…¨å€¼
- rerank: string (å¯é€‰) - é‡æ’åºç­–ç•¥ï¼š

  * "pass": è·³è¿‡é‡æ’ï¼Œä½¿ç”¨åŸå§‹å‘é‡ç›¸ä¼¼åº¦æ’åºï¼ˆé»˜è®¤ï¼‰
    - ä¼˜ç‚¹ï¼šé€Ÿåº¦å¿«ï¼Œè¿”å›ç»“æœå°±æ˜¯å‘é‡ç›¸ä¼¼åº¦æ’åº

  * "degree": åŸºäºå€™é€‰å­å›¾åº¦æ•°é‡æ’åºï¼ˆæ¨èç”¨äºçº æ­£å‘é‡åå·®ï¼‰
    - ä¼˜ç‚¹ï¼šåæ˜ å®ä½“åœ¨å€™é€‰å­å›¾çš„é‡è¦æ€§

  * "pagerank": åŸºäºå…¨å›¾ PageRank é‡æ’åºï¼ˆæ¨èç”¨äºæ‰¾é‡è¦å®ä½“ï¼‰
    - ä¼˜ç‚¹ï¼šåæ˜ å®ä½“åœ¨æ•´ä¸ªçŸ¥è¯†å›¾è°±ä¸­çš„é‡è¦æ€§

è¿”å›æ ¼å¼:
JSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
{
  "entity_name": "å®ä½“åç§°",
  "description": "å®ä½“æè¿°",
  "labels": ["æ ‡ç­¾1", "æ ‡ç­¾2", ...],
  "file_path": "æºæ–‡ä»¶è·¯å¾„",
  "created_at": "åˆ›å»ºæ—¶é—´",
  // å½“rerank="degree"æ—¶è¿˜ä¼šåŒ…å«:
  "degree": "åœ¨å€™é€‰å­å›¾ä¸­çš„åº¦æ•°ï¼ˆè¿æ¥æ•°ï¼‰",
  "similarity_score": "åŸå§‹å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°"
  // å½“rerank="pagerank"æ—¶è¿˜ä¼šåŒ…å«:
  "pagerank": "å…¨å›¾ PageRank å€¼ï¼ˆé‡è¦æ€§ï¼‰",
  "similarity_score": "åŸå§‹å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°"
}

ä½¿ç”¨ç¤ºä¾‹:
1. æŸ¥æ‰¾ä¸"ç¥ç»ç½‘ç»œ"ç›¸ä¼¼çš„å®ä½“: {"entity_query": "ç¥ç»ç½‘ç»œ", "top_k": 10}
2. æŸ¥æ‰¾"Python"ç›¸å…³çš„å®ä½“: {"entity_query": "Pythonç¼–ç¨‹è¯­è¨€"}
3. æŸ¥æ‰¾"æ•°æ®åº“"ç›¸å…³å®ä½“: {"entity_query": "æ•°æ®åº“ç®¡ç†ç³»ç»Ÿ"}
4. å¯ç”¨åº¦æ•°é‡æ’çº æ­£åå·®: {"entity_query": "LCT", "top_k": 30, "rerank": "degree"}
5. å¯ç”¨ PageRank é‡æ’æ‰¾é‡è¦å®ä½“: {"entity_query": "å›¾ç®—æ³•", "top_k": 20, "rerank": "pagerank"}

é‡æ’ç­–ç•¥é€‰æ‹©å»ºè®®:
- ä¸€èˆ¬æŸ¥è¯¢ï¼šä½¿ç”¨ "pass" æˆ– "degree"
- å¯»æ‰¾é‡è¦æ¦‚å¿µï¼šä½¿ç”¨ "pagerank"
- çº æ­£å‘é‡åå·®ï¼šä½¿ç”¨ "degree"
"""
)
async def find_similar_entities(
    entity_query: str,
    top_k: int = 5,
    rerank: str = "pass",
) -> List[Dict[str, Any]]:
    """æŸ¥æ‰¾ç›¸ä¼¼çš„å®ä½“"""
    # å®‰å…¨æ§åˆ¶ï¼šéªŒè¯ top_k å‚æ•°ï¼Œé˜²æ­¢ DoS æ”»å‡»
    SAFE_DEFAULT_TOP_K = 10
    MAX_TOP_K = 30

    if top_k is None or top_k <= 0 or top_k > MAX_TOP_K:
        top_k = SAFE_DEFAULT_TOP_K if top_k is None or top_k <= 0 else MAX_TOP_K
        logger.info(f"[SECURITY] top_k adjusted to safe value: {top_k}")

    # ç›´æ¥è°ƒç”¨ï¼Œé¿å… asyncio.wait_for ä¸ FastMCP äº‹ä»¶å¾ªç¯å†²çª
    problems = await service.find_similar_entities(
        entity_query=entity_query,
        top_k=top_k,
        rerank=rerank,
    )
    # è¿”å›åŸå§‹æ•°æ®ï¼Œè®©FastMCPè‡ªåŠ¨å¤„ç†åºåˆ—åŒ–
    return problems
@mcp.tool(
    name="execute_custom_cypher",
    description="""é«˜çº§ Cypher æŸ¥è¯¢å·¥å…· - æ”¯æŒé«˜åº¦è‡ªå®šä¹‰çš„çµæ´»æŸ¥è¯¢ï¼ˆæœ€æ¨èï¼‰

ã€é‡è¦æç¤ºã€‘ï¼šè¯·ä¸è¦åœ¨è¿”å›ç»“æœä¸­åŒ…å« embedding å‘é‡æˆ–ä»»ä½•å‘é‡æ•°æ®å­—æ®µï¼
å¦‚æœæŸ¥è¯¢å®ä½“èŠ‚ç‚¹ï¼Œåªè¿”å›æ–‡æœ¬å±æ€§ï¼šentity_name, description, labels, file_path, created_at ç­‰ã€‚

ã€æ ¸å¿ƒä¼˜åŠ¿ã€‘ï¼š
- æ”¯æŒå¤šçº§å…³ç³»æŸ¥è¯¢ï¼ˆ1-3è·³ï¼‰ï¼Œæ·±åº¦æŒ–æ˜çŸ¥è¯†å›¾è°±
- æ”¯æŒå‘é‡æœç´¢ + å¤šçº§å…³ç³»çš„ç»„åˆæŸ¥è¯¢ï¼Œæ•ˆæœæ˜¾è‘—æå‡
- æ”¯æŒå¤šæºæŸ¥è¯¢ã€å¤æ‚ç­›é€‰ï¼Œæ»¡è¶³å„ç§åœºæ™¯éœ€æ±‚

âš¡ æŸ¥è¯¢ç­–ç•¥æŒ‡å—ï¼š

ã€ä¸€çº§å…³ç³»æŸ¥è¯¢ã€‘ï¼ˆ<1ç§’ï¼‰- é€‚ç”¨äºï¼šç›´æ¥ç›¸å…³æ¦‚å¿µã€ç®€å•æŸ¥è¯¢
- å‘é‡æœç´¢ + ç›´æ¥é‚»å±…
- ç²¾ç¡®åŒ¹é… + ç›´æ¥å…³ç³»

ã€äºŒçº§å…³ç³»æŸ¥è¯¢ã€‘ï¼ˆ~3-4ç§’ï¼‰- é€‚ç”¨äºï¼šç›¸å…³æ‰©å±•ã€é¢˜ç›®æŸ¥æ‰¾ï¼ˆæ¨èï¼‰
- å‘é‡æœç´¢ + äºŒçº§å…³ç³»ï¼ˆç¤ºä¾‹6ï¼‰
- ç²¾ç¡®ID + äºŒçº§å…³ç³»

ã€å¤šæºé‚»åŸŸäº¤é›†ã€‘ï¼ˆ~5-6ç§’ï¼‰- é€‚ç”¨äºï¼šç»¼åˆé¢˜ç›®ã€è·¨é¢†åŸŸåˆ†æ
- å¤šæºå‘é‡æœç´¢ + é‚»åŸŸäº¤é›†ï¼ˆç¤ºä¾‹7ï¼‰

ğŸ¯ åœºæ™¯æ¨èï¼š
- æŸ¥æ‰¾ç›¸ä¼¼é¢˜ç›®ï¼šä½¿ç”¨"å‘é‡æœç´¢ + äºŒçº§å…³ç³»"ï¼ˆç¤ºä¾‹6ï¼‰
- ç»¼åˆåˆ†æï¼šä½¿ç”¨"å¤šæºå‘é‡æœç´¢ + é‚»åŸŸäº¤é›†"ï¼ˆç¤ºä¾‹7ï¼‰
- å·²çŸ¥å®ä½“æ‰©å±•ï¼šä½¿ç”¨"ç²¾ç¡®ID + å¤šçº§å…³ç³»"ï¼ˆç¤ºä¾‹5ï¼‰

ğŸ’¡ æ€§èƒ½æç¤ºï¼š5-6ç§’çš„æŸ¥è¯¢æ—¶é—´æ˜¯å¯æ¥å—çš„ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æŸ¥è¯¢æ•ˆæœï¼

è¾“å…¥å‚æ•°:
- query: string (å¿…éœ€) - Cypher æŸ¥è¯¢è¯­å¥ï¼Œå¿…é¡»ä½¿ç”¨ $å‚æ•°å æ ¼å¼
- parameters: object (å¿…éœ€) - æŸ¥è¯¢å‚æ•°å­—å…¸
- vector_params: object (å¿…éœ€) - å‘é‡å‚æ•°æ˜ å°„ï¼Œæ ‡è®°å“ªäº›å‚æ•°éœ€è¦è½¬æ¢ä¸ºå‘é‡è¿›è¡Œæ¨¡ç³Šæœç´¢ã€‚trueä»£è¡¨åµŒå…¥æˆå‘é‡ï¼Œfalseä»£è¡¨ç²¾ç¡®åŒ¹é…
  ä¾‹: {"query_vector": true} è¡¨ç¤ºå°† parameters.query_vector çš„å­—ç¬¦ä¸²è½¬æ¢ä¸ºå‘é‡
- limit: integer (å¯é€‰) - ç»“æœæ•°é‡é™åˆ¶ï¼ŒèŒƒå›´ 1-30ï¼Œè¶…å‡ºèŒƒå›´å°†è‡ªåŠ¨è°ƒæ•´ä¸ºå®‰å…¨å€¼

âš ï¸ å®‰å…¨æ§åˆ¶:
- ç³»ç»Ÿä¼šè‡ªåŠ¨éªŒè¯ limit å‚æ•°ï¼Œé˜²æ­¢ DoS æ”»å‡»
- å¦‚æœä¼ å…¥è´Ÿæ•°ã€é›¶æˆ– Noneï¼Œå°†ä½¿ç”¨å®‰å…¨é»˜è®¤å€¼
- å¦‚æœè¶…å‡ºæœ€å¤§å€¼ï¼ˆ30ï¼‰ï¼Œå°†è¢«å¼ºåˆ¶é™åˆ¶ä¸ºæœ€å¤§å€¼

è¿”å›æ ¼å¼:
JSONæ•°ç»„ï¼Œæ¯è¡ŒåŒ…å«æŸ¥è¯¢ç»“æœè®°å½•

================================================================

ç¤ºä¾‹1: ä¸€çº§å…³ç³»æŸ¥è¯¢ï¼ˆå¿«é€Ÿï¼‰

æŸ¥è¯¢ä¸Splayæ ‘ç›¸å…³çš„æŠ€å·§æœ‰å“ªäº›ï¼š
{
  "query": "CALL db.index.vector.queryNodes('entity_embedding_index', 20, $query_vector) YIELD node as splay_node WHERE 'æ•°æ®ç»“æ„' IN labels(splay_node) MATCH (splay_node)-[r:RELATED_TO]-(technique) WHERE 'æŠ€å·§' IN labels(technique) RETURN technique.entity_id as entity_name, technique.description as description, labels(technique) as labels, technique.file_path as file_path, r.weight as weight, r.keywords as relationship_type, splay_node.entity_id as related_to ORDER BY weight DESC",
  "parameters": {"query_vector": "Splayæ ‘ï¼Œä¸€ç§å¹³è¡¡äºŒå‰æŸ¥æ‰¾æ ‘"},
  "vector_params": {"query_vector": true},
  "limit": 20
}

1. è¿‡æ»¤å‡º"æ•°æ®ç»“æ„"ç±»å‹çš„èŠ‚ç‚¹
2. æŸ¥æ‰¾è¿™äº›èŠ‚ç‚¹çš„é‚»å±…ä¸­å¸¦æœ‰"æŠ€å·§"æ ‡ç­¾çš„èŠ‚ç‚¹
3. æ‰€æœ‰å…³ç³»ç±»å‹ç»Ÿä¸€ä¸ºRELATED_TOï¼ŒçœŸå®ç±»å‹å­˜å‚¨åœ¨keywordså±æ€§ä¸­

================================================================

ç¤ºä¾‹2: æŸ¥è¯¢å®ä½“å…³ç³»

æŸ¥è¯¢æŸä¸ªå®ä½“çš„æ‰€æœ‰å…³ç³»ï¼š
{
  "query": "MATCH (e {entity_id: $name})-[r:RELATED_TO]->(target) RETURN e.entity_id as source_entity, r.keywords as relationship_type, r.description as description, r.weight as weight, target.entity_id as target_entity, target ORDER BY r.weight DESC",
  "parameters": {"name": "çº¿æ®µæ ‘"},
  "vector_params": {"name": false},
  "limit": 20
}



================================================================

ç¤ºä¾‹3: é¢˜ç›®å…³è”æŸ¥è¯¢ï¼ˆäºŒçº§é‚»å±…æœç´¢ï¼‰

æŸ¥æ‰¾æ‰€æœ‰å’ŒGem Islandç›¸ä¼¼çš„é¢˜ç›®ï¼š
{
  "query": "MATCH (problem) WHERE 'é¢˜ç›®' IN labels(problem) AND problem.entity_id CONTAINS $problem_name MATCH (problem)-[r1:RELATED_TO]-(neighbor) MATCH (neighbor)-[r2:RELATED_TO]-(second_neighbor) WHERE 'é¢˜ç›®' IN labels(second_neighbor) RETURN second_neighbor.entity_id as entity_name, second_neighbor.description as description, labels(second_neighbor) as labels, second_neighbor.file_path as file_path, neighbor.entity_id as first_level_neighbor, r1.weight as r1_weight, r2.weight as r2_weight, (r1.weight + r2.weight) as total_weight ORDER BY total_weight DESC",
  "parameters": {"problem_name": "Gem Island"},
  "vector_params": {"problem_name": false},
  "limit": 30
}

1. æŸ¥æ‰¾è¿™äº›èŠ‚ç‚¹çš„ç›´æ¥é‚»å±…ï¼ˆä¸€çº§å…³ç³»ï¼‰
2. æŸ¥æ‰¾é‚»å±…çš„é‚»å±…ï¼ˆäºŒçº§å…³ç³»ï¼‰
3. è¿‡æ»¤å‡ºå¸¦æœ‰"é¢˜ç›®"æ ‡ç­¾çš„äºŒçº§é‚»å±…èŠ‚ç‚¹

================================================================

ç¤ºä¾‹4: å¤æ‚æŸ¥è¯¢ - æ‰¾åˆ°åŒæ—¶ä¸ä¸¤ä¸ªå®ä½“ç›¸é‚»çš„èŠ‚ç‚¹

æŸ¥è¯¢æ—¢å¯ä»¥ç”¨çº¿æ®µæ ‘ä¹Ÿå¯ä»¥ç”¨æ ‘çŠ¶æ•°ç»„è§£å†³çš„é¢˜ç›®ï¼ˆä½¿ç”¨å‘é‡æœç´¢ï¼‰ï¼š
{
  "query": "CALL db.index.vector.queryNodes('entity_embedding_index', 20, $query_vector1) YIELD node as e1 CALL db.index.vector.queryNodes('entity_embedding_index', 20, $query_vector2) YIELD node as e2 WITH collect(DISTINCT e1) as e1_list, collect(DISTINCT e2) as e2_list MATCH (e1_node)-[r1:RELATED_TO]-(common), (e2_node)-[r2:RELATED_TO]-(common) WHERE e1_node IN e1_list AND e2_node IN e2_list RETURN common.entity_id as entity_name, common.description as description, labels(common) as labels, (r1.weight + r2.weight) as total_weight ORDER BY total_weight DESC",
  "parameters": {"query_vector1": "çº¿æ®µæ ‘", "query_vector2": "æ ‘çŠ¶æ•°ç»„"},
  "vector_params": {"query_vector1": true, "query_vector2": true},
  "limit": 20
}

ä¼˜åŒ–è¯´æ˜ï¼š
- ä½¿ç”¨è¾ƒå¤§çš„top_kï¼ˆå¦‚20ï¼‰æ‰©å¤§æœç´¢èŒƒå›´ï¼Œé¿å…é—æ¼ç›¸å…³èŠ‚ç‚¹
- é€šè¿‡WITHè¯­å¥æ”¶é›†å€™é€‰èŠ‚ç‚¹ï¼Œå†è¿›è¡Œå…³ç³»æŸ¥è¯¢
- å¯ä»¥æ ¹æ®éœ€è¦è¿‡æ»¤ç»“æœä¸­çš„"é¢˜ç›®"ç±»å‹èŠ‚ç‚¹

================================================================

ç¤ºä¾‹5: ç²¾ç¡®IDçš„é‚»åŸŸå…±åŒé¢˜ç›®æŸ¥è¯¢

å·²çŸ¥ä¸¤ä¸ªå®ä½“çš„ç²¾ç¡®IDï¼ŒæŸ¥æ‰¾å®ƒä»¬1-2çº§é‚»åŸŸä¸­å…±æœ‰çš„"é¢˜ç›®"ç±»å‹èŠ‚ç‚¹ï¼š
{
  "query": "MATCH path1=(e1 {entity_id: $entity_id1})-[*1..2:RELATED_TO]-(common) MATCH path2=(e2 {entity_id: $entity_id2})-[*1..2:RELATED_TO]-(common) WHERE 'é¢˜ç›®' IN labels(common) AND common <> e1 AND common <> e2 RETURN DISTINCT common.entity_id as entity_name, common.description as description, labels(common) as labels, common.file_path as file_path, length(path1) as dist_from_e1, length(path2) as dist_from_e2 ORDER BY (length(path1) + length(path2)) ASC",
  "parameters": {"entity_id1": "çº¿æ®µæ ‘<QyCKb7>", "entity_id2": "æ ‘çŠ¶æ•°ç»„<ABC123>"},
  "vector_params": {"entity_id1": false, "entity_id2": false},
  "limit": 20
}

æ³¨æ„ï¼šä½¿ç”¨[*1..2:RELATED_TO]æŒ‡å®šå…³ç³»ç±»å‹ï¼Œæ‰€æœ‰å…³ç³»ç±»å‹ç»Ÿä¸€ä¸ºRELATED_TO

================================================================

ç¤ºä¾‹6: **å‘é‡æœç´¢ + äºŒçº§å…³ç³» + pagerank é‡æ’**ï¼ˆæ¨èç”¨äºæŸ¥æ‰¾ç›¸å…³é¢˜ç›®ï¼Œ~3-4ç§’ï¼‰

æŸ¥è¯¢ä¸"çº¿æ®µæ ‘ç»´æŠ¤åŒºé—´"ç›¸å…³çš„é¢˜ç›®ï¼Œå¹¶æŒ‰ pagerank é‡æ’ï¼š
```json
{
  "query": "CALL db.index.vector.queryNodes('entity_embedding_index', 25, $query_vector) YIELD node as concept_node
  MATCH (concept_node)-[r1:RELATED_TO]-(mid)
  MATCH (mid)-[r2:RELATED_TO]-(problem)
  WHERE 'é¢˜ç›®' IN labels(problem)
  WITH DISTINCT problem, concept_node, mid, r1, r2,
       (r1.weight * 0.4 + r2.weight * 0.6) as total_score
  // æŒ‰ pagerank å’Œå‘é‡ç›¸ä¼¼åº¦ç»¼åˆæ’åºï¼ˆæ³¨æ„ï¼šDISTINCT å»é‡é˜²æ­¢åŒä¸€é¢˜ç›®è¢«å¤šæ¬¡è¿”å›ï¼‰
  RETURN problem.entity_id as entity_name, problem.description as description,
         labels(problem) as labels,
         problem.file_path as file_path,
         problem.pagerank as pagerank,
         concept_node.entity_id as concept, mid.entity_id as intermediate,
         r1.weight as r1_weight, r2.weight as r2_weight, total_score
  ORDER BY problem.pagerank DESC, total_score DESC",
  "parameters": {"query_vector": "çº¿æ®µæ ‘ç»´æŠ¤åŒºé—´"},
  "vector_params": {"query_vector": true},
  "limit": 30
}
```

âš ï¸ é‡è¦ï¼šå¿…é¡»ä½¿ç”¨ `WITH DISTINCT` å»é‡ï¼å¦åˆ™é€šè¿‡å¤šä¸ªä¸­é—´èŠ‚ç‚¹ç›¸è¿çš„åŒä¸€é¢˜ç›®ä¼šè¢«è¿”å›å¤šæ¬¡ã€‚

é€‚ç”¨åœºæ™¯ï¼šæŸ¥æ‰¾ä¸æŸä¸ªæ¦‚å¿µç›¸å…³çš„é¢˜ç›®ï¼Œé€šè¿‡ä¸­é—´èŠ‚ç‚¹è¿æ¥åæŒ‰ pagerank é‡æ’
æ€§èƒ½ï¼š~3-4ç§’ | ä¼˜åŠ¿ï¼šæŒ‰å®ä½“å…¨å±€é‡è¦æ€§æ’åºï¼Œä¼˜å…ˆè¿”å›æ ¸å¿ƒé¢˜ç›®
æ³¨æ„ï¼šèŠ‚ç‚¹ pagerank å€¼åæ˜ å…¶åœ¨æ•´ä¸ªçŸ¥è¯†å›¾è°±ä¸­çš„é‡è¦æ€§ï¼Œå¯ä½œä¸ºæ’åºä¾æ®

================================================================

ç¤ºä¾‹7: **å¤šæºå‘é‡æœç´¢ + é‚»åŸŸäº¤é›† + pagerank é‡æ’**ï¼ˆæ¨èç”¨äºæŸ¥æ‰¾ç»¼åˆé¢˜ç›®ï¼Œ~5-6ç§’ï¼‰

æ£€ç´¢åŒæ—¶å’Œ"åŠ¨æ€è§„åˆ’ï¼Œç»„åˆæ•°å­¦"æœ‰å…³çš„é¢˜ç›®ï¼Œå¹¶æŒ‰ pagerank é‡æ’ï¼š
```json
{
  "query": "CALL db.index.vector.queryNodes('entity_embedding_index', 20, $query_vector1) YIELD node as dp_node
  CALL db.index.vector.queryNodes('entity_embedding_index', 20, $query_vector2) YIELD node as math_node
  WITH collect(DISTINCT dp_node) as dp_nodes, collect(DISTINCT math_node) as math_nodes

  // æ”¶é›†dpèŠ‚ç‚¹çš„ä¸€äºŒçº§é‚»åŸŸï¼ˆæ³¨æ„ï¼šæ¯ä¸€æ­¥éƒ½ä½¿ç”¨ DISTINCT/collect(DISTINCT) å»é‡ï¼‰
  MATCH (dp)-[r1:RELATED_TO]-(dp_neighbor1)
  WHERE dp IN dp_nodes
  WITH collect(DISTINCT dp_neighbor1) as dp_neighbors1, dp_nodes, math_nodes
  MATCH (dp2)-[r2:RELATED_TO]-(dp_neighbor2)
  WHERE dp2 IN dp_nodes OR dp2 IN dp_neighbors1
  WITH collect(DISTINCT dp_neighbor2) as dp_all_neighbors, math_nodes

  // æ”¶é›†ç»„åˆæ•°å­¦èŠ‚ç‚¹çš„ä¸€äºŒçº§é‚»åŸŸ
  MATCH (math)-[r3:RELATED_TO]-(math_neighbor1)
  WHERE math IN math_nodes
  WITH collect(DISTINCT math_neighbor1) as math_neighbors1, dp_all_neighbors, math_nodes
  MATCH (math2)-[r4:RELATED_TO]-(math_neighbor2)
  WHERE math2 IN math_nodes OR math2 IN math_neighbors1
  WITH collect(DISTINCT math_neighbor2) as math_all_neighbors, dp_all_neighbors

  // å–äº¤é›†å¹¶è¿‡æ»¤å‡ºé¢˜ç›®æ ‡ç­¾ï¼ˆä½¿ç”¨ DISTINCT ç¡®ä¿äº¤é›†ä¸­æ²¡æœ‰é‡å¤ï¼‰
  MATCH (candidate)
  WHERE candidate IN dp_all_neighbors AND candidate IN math_all_neighbors
    AND 'é¢˜ç›®' IN labels(candidate)
  WITH DISTINCT candidate
  RETURN candidate.entity_id as entity_name, candidate.description as description,
         labels(candidate) as labels,
         candidate.file_path as file_path,
         candidate.pagerank as pagerank
  ORDER BY candidate.pagerank DESC, candidate.entity_id ASC",
  "parameters": {"query_vector1": "åŠ¨æ€è§„åˆ’", "query_vector2": "ç»„åˆæ•°å­¦"},
  "vector_params": {"query_vector1": true, "query_vector2": true},
  "limit": 25
}
```

âš ï¸ é‡è¦ï¼šå¤šçº§é‚»åŸŸæŸ¥è¯¢å¿…é¡»æ¯ä¸€æ­¥éƒ½å»é‡ï¼
- ä½¿ç”¨ `collect(DISTINCT ...)` æ”¶é›†å”¯ä¸€èŠ‚ç‚¹
- æœ€ç»ˆç»“æœå‰åŠ  `WITH DISTINCT candidate` ç¡®ä¿äº¤é›†ç»“æœæ— é‡å¤

é€‚ç”¨åœºæ™¯ï¼šæŸ¥æ‰¾åŒæ—¶æ¶‰åŠå¤šä¸ªé¢†åŸŸçš„ç»¼åˆé¢˜ç›®ï¼Œå‘ç°è·¨é¢†åŸŸå…³è”åæŒ‰ pagerank é‡æ’
æ€§èƒ½ï¼š~5-6ç§’ | ä¼˜åŠ¿ï¼šå‘ç°æ·±å±‚å…³è”çš„é¢˜ç›®ï¼Œä¼˜å…ˆè¿”å›æ ¸å¿ƒé‡è¦é¢˜ç›®
æ³¨æ„ï¼šè¿”å›ç»“æœåŒ…å« pagerank å­—æ®µï¼Œå¯ç”¨äºåç»­åˆ†æ

================================================================

å›¾è°± Schema è¯¦ç»†è¯´æ˜ï¼š

ã€å®ä½“ç±»å‹ - ç¬¬ä¸€ç»´åº¦ï¼ˆæŠ€æœ¯åˆ†ç±»ï¼‰ã€‘
- æ•°æ®ç»“æ„ï¼šæ•°æ®ç»“æ„ç›¸å…³æ¦‚å¿µ
- ç®—æ³•æ€æƒ³ï¼šé€šç”¨ç®—æ³•æ€æƒ³å’Œç­–ç•¥
- åŠ¨æ€è§„åˆ’ï¼šåŠ¨æ€è§„åˆ’ç›¸å…³å†…å®¹
- å›¾è®ºï¼šå›¾è®ºç®—æ³•å’Œæ¦‚å¿µ
- æœç´¢ï¼šæœç´¢ç®—æ³•å’Œç­–ç•¥
- å­—ç¬¦ä¸²ï¼šå­—ç¬¦ä¸²å¤„ç†ç›¸å…³
- æ•°å­¦ï¼šæ•°å­¦çŸ¥è¯†å’Œå®šç†
- è®¡ç®—å‡ ä½•ï¼šå‡ ä½•ç®—æ³•

ã€å®ä½“ç±»å‹ - ç¬¬äºŒç»´åº¦ï¼ˆåº”ç”¨å±‚æ¬¡ï¼‰ã€‘
- æ¦‚å¿µï¼šæŠ½è±¡æ¦‚å¿µå’Œå®šä¹‰
- æŠ€å·§ï¼šè§£é¢˜æŠ€å·§å’ŒTrick
- å®ç°ï¼šå…·ä½“å®ç°æ–¹æ³•
- æ¨¡å‹ï¼šæ•°å­¦æ¨¡å‹å’ŒæŠ½è±¡æ¨¡å‹
- ç®—æ³•ï¼šå…·ä½“ç®—æ³•åç§°
- åŸç†ï¼šåŸç†å’Œç†è®º
- é¢˜ç›®ï¼šå…·ä½“é¢˜ç›®å’Œå®ä¾‹

ã€å…³ç³»ç±»å‹ï¼ˆ11ç§æ ‡å‡†ç±»å‹ï¼‰ã€‘
- IS_Aï¼šåˆ†ç±»å…³ç³»ï¼ˆXæ˜¯Yçš„ä¸€ç§/ä¸€ç±»/å®ä¾‹ï¼‰
- PART_OFï¼šç»„æˆå…³ç³»ï¼ˆåŒ…å«ã€æ„æˆã€åˆ†è§£ï¼‰
- BASED_ONï¼šä¾èµ–å…³ç³»ï¼ˆåŸºäºã€ä¾èµ–ã€å‰æã€åŸç†ï¼‰
- APPLIES_TOï¼šåº”ç”¨å…³ç³»ï¼ˆåº”ç”¨ã€è§£å†³ã€å¤„ç†ï¼‰
- EVALUATESï¼šè¯„ä¼°å…³ç³»ï¼ˆéªŒè¯ã€æµ‹è¯•ï¼‰
- EXPLAINSï¼šè§£é‡Šå…³ç³»ï¼ˆåˆ†æã€é˜æ˜ï¼‰
- PRACTICED_BYï¼šå®è·µå…³ç³»ï¼ˆåº”ç”¨äºé¢˜ç›®åœºæ™¯ï¼‰
- COMPARES_WITHï¼šå¯¹æ¯”å…³ç³»ï¼ˆå¯¹æ¯”ã€å…³è”ã€ç±»ä¼¼ã€æ›¿ä»£ï¼‰
- LEADS_TOï¼šæ¨å¯¼å…³ç³»ï¼ˆæ¨å¯¼ã€è½¬åŒ–ã€å¯¼è‡´ï¼‰
- OPTIMIZESï¼šä¼˜åŒ–å…³ç³»ï¼ˆä¼˜åŒ–ã€ç®€åŒ–ã€åŠ é€Ÿï¼‰
- TRANSFORMS_TOï¼šè½¬æ¢å…³ç³»ï¼ˆè½¬åŒ–ã€è½¬æ¢ã€æ˜ å°„ï¼‰

Neo4j æ•°æ®å­˜å‚¨ç»“æ„ï¼š

å®ä½“å­˜å‚¨ï¼š
- èŠ‚ç‚¹æ ‡ç­¾ï¼šç›´æ¥ä½¿ç”¨å¤šä¸ªç±»å‹æ ‡ç­¾ï¼ˆå¦‚ï¼š'æ•°æ®ç»“æ„', 'æŠ€å·§', 'é¢˜ç›®'ï¼‰ï¼Œä¸å†æœ‰'Entity'é€šç”¨æ ‡ç­¾
- èŠ‚ç‚¹å±æ€§ï¼šentity_id, description, file_path, created_at
- æ­£ç¡®è·å–ç±»å‹ï¼šç›´æ¥ä½¿ç”¨labels(node)è¿”å›æ‰€æœ‰ç±»å‹æ ‡ç­¾æ•°ç»„

å…³ç³»å­˜å‚¨ï¼š
- å…³ç³»ç±»å‹ï¼šæ‰€æœ‰å…³ç³»ç»Ÿä¸€ä¸ºRELATED_TOç±»å‹
- çœŸå®å…³ç³»ç±»å‹ï¼šå­˜å‚¨åœ¨r.keywordså±æ€§ä¸­ï¼ˆå¤šä¸ªå…³é”®è¯ç”¨é€—å·åˆ†éš”ï¼Œå¦‚"IS_A,BASED_ON"ï¼‰
- å…³ç³»å±æ€§ï¼šdescription, weight, keywordsï¼ˆçœŸå®å…³ç³»ç±»å‹åˆ—è¡¨ï¼‰
- å…³ç³»æ–¹å‘ï¼šæ”¯æŒ OUTGOING (->) å’Œ INCOMING (<-)
- æŸ¥è¯¢å…³ç³»ç±»å‹ï¼šä½¿ç”¨ r.keywords CONTAINS 'IS_A' è€Œé type(r) = 'IS_A'

================================================================

æœ€ä½³å®è·µï¼š

ã€æŸ¥è¯¢ç­–ç•¥é€‰æ‹©ã€‘
1. ç®€å•æŸ¥è¯¢ï¼ˆ<1ç§’ï¼‰ï¼šç›´æ¥ä½¿ç”¨ä¸€çº§å…³ç³»æŸ¥è¯¢ï¼ˆç¤ºä¾‹1-2ï¼‰
   - é€‚ç”¨ï¼šæŸ¥æ‰¾ç›´æ¥ç›¸å…³æ¦‚å¿µã€ç²¾ç¡®åŒ¹é…å·²çŸ¥å®ä½“

2. æ·±åº¦æŸ¥è¯¢ï¼ˆ3-6ç§’ï¼‰ï¼šä½¿ç”¨äºŒçº§å…³ç³»æŸ¥è¯¢ï¼ˆç¤ºä¾‹6-7ï¼‰
   - é€‚ç”¨ï¼šæŸ¥æ‰¾é—´æ¥ç›¸å…³é¢˜ç›®ã€å¯¹æ¯”åˆ†æã€æ·±åº¦æ¢ç´¢
   - æ€§èƒ½æç¤ºï¼š5-6ç§’çš„æŸ¥è¯¢æ—¶é—´æ˜¯å¯æ¥å—çš„ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡æŸ¥è¯¢æ•ˆæœï¼

3. å¤æ‚åœºæ™¯ï¼ˆ5-6ç§’ï¼‰ï¼šä½¿ç”¨å¤šæºæŸ¥è¯¢ + å¤šçº§å…³ç³» + äº¤é›†ï¼ˆç¤ºä¾‹7ï¼‰
   - é€‚ç”¨ï¼šå…¨é¢åˆ†æã€å¤æ‚åœºæ™¯

ã€å‘é‡æœç´¢æœ€ä½³å®è·µã€‘
1. ä¼˜å…ˆä½¿ç”¨å‘é‡æœç´¢è¿›è¡Œæ¨¡ç³ŠæŸ¥è¯¢ï¼Œæ•ˆæœæ›´å¥½
2. ä½¿ç”¨è¾ƒå¤§çš„top_kï¼ˆå¦‚20-30ï¼‰æ‰©å¤§æœç´¢èŒƒå›´ï¼Œé¿å…é—æ¼ç›¸å…³èŠ‚ç‚¹
3. vector_params ä¸­è®¾ç½®ä¸º true çš„å‚æ•°ä¼šè‡ªåŠ¨è½¬æ¢ä¸ºå‘é‡
4. ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ï¼ŒæŸ¥è¯¢çš„å­—ç¬¦ä¸²æè¿°åº”è¯¥å°½é‡å‡†ç¡®ï¼Œä½¿ç”¨"åŠ¨æ€è§„åˆ’ï¼Œä¸€ç§ç”¨äºæ±‚è§£å¤æ‚é—®é¢˜çš„ç®—æ³•æ€æƒ³"æ¯”ç›´æ¥æŸ¥è¯¢"åŠ¨æ€è§„åˆ’"æ•ˆæœæ›´å¥½

ã€å·¥ä½œæµç¨‹å»ºè®®ã€‘
1. æ¢ç´¢é˜¶æ®µï¼šä½¿ç”¨ find_similar_entities å¿«é€Ÿæ‰¾åˆ°ç›¸å…³å®ä½“
2. ç²¾ç¡®æŸ¥è¯¢ï¼šä½¿ç”¨ execute_custom_cypher è¿›è¡Œæ·±åº¦æŒ–æ˜
3. å¯ä»¥å…ˆä½¿ç”¨ find_similar_entities å·¥å…·æ‰¾åˆ°ä¸€ä¸ªç¡®å®šèŠ‚ç‚¹çš„IDï¼Œå†ä½¿ç”¨Cypherå·¥å…·ç²¾ç¡®åŒ¹é…èŠ‚ç‚¹ID
4. å¤šæ­¥æŸ¥è¯¢ï¼šå…ˆä½¿ç”¨ç®€å•æŸ¥è¯¢ç¡®è®¤æ–¹å‘ï¼Œå†ä½¿ç”¨å¤æ‚æŸ¥è¯¢æ·±å…¥æŒ–æ˜

ã€æ€§èƒ½ä¼˜åŒ–ã€‘
1. æŸ¥è¯¢ä¼šè‡ªåŠ¨æ·»åŠ  LIMIT é™åˆ¶é˜²æ­¢è¿‡è½½
2. å¯¹äºå¤æ‚æŸ¥è¯¢ï¼Œå¯ä»¥ä½¿ç”¨åˆ†æ­¥æŸ¥è¯¢é¿å…è¶…æ—¶

ã€æŒ‰éœ€ä½¿ç”¨ç­–ç•¥ã€‘
- ä¸è¦æ€»æ˜¯ä¿å®ˆé€‰æ‹©å¼€é”€å°çš„æŸ¥è¯¢ï¼æ ¹æ®æŸ¥è¯¢éœ€æ±‚é€‰æ‹©åˆé€‚çš„æŸ¥è¯¢æ·±åº¦
- ä¸€äºŒçº§é‚»åŸŸæŸ¥è¯¢è™½ç„¶æ¶ˆè€—æ›´å¤§ï¼Œä½†æ•ˆæœæ˜¾è‘—æå‡
- ç»“åˆåµŒå…¥å‘é‡ç›¸ä¼¼topkå’ŒäºŒçº§é‚»åŸŸçš„é«˜çº§æŸ¥è¯¢æ‰‹æ®µå€¼å¾—ä½¿ç”¨ï¼ˆ5-6sï¼‰
- LLMåº”è¯¥æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦æŒ‰éœ€é€‰æ‹©æŸ¥è¯¢æ‰‹æ®µï¼Œè€Œä¸æ˜¯æ¯æ¬¡éƒ½ä¿å®ˆä½¿ç”¨å¼€é”€å°çš„æŸ¥è¯¢
"""
)
async def execute_custom_cypher(
    query: str,
    parameters: Optional[Dict[str, Any]] = None,
    limit: Optional[int] = None,
    vector_params: Optional[Dict[str, bool]] = None,
) -> List[Dict[str, Any]]:
    """æ‰§è¡Œè‡ªå®šä¹‰ Cypher æŸ¥è¯¢"""

    # ç›´æ¥è°ƒç”¨ï¼Œé¿å… asyncio.wait_for ä¸ FastMCP äº‹ä»¶å¾ªç¯å†²çª
    # æ³¨æ„ï¼šç»“æœå·²ç»åœ¨ kg_query_engine.execute_cypher ä¸­è‡ªåŠ¨è¿‡æ»¤äº† embedding å­—æ®µ
    results = await service.execute_custom_cypher(
        query=query,
        parameters=parameters,
        limit=limit,
        vector_params=vector_params,
    )

    # è¿”å›è¿‡æ»¤åçš„æ•°æ®ï¼Œè®©FastMCPè‡ªåŠ¨å¤„ç†åºåˆ—åŒ–
    return results

@mcp.tool(
    name="get_chunks",
    description="""è·å–å®ä½“å…³è”çš„ Chunk åˆ—è¡¨å·¥å…·

æ ¹æ®å®ä½“çš„ç²¾ç¡® ID è·å–è¯¥å®ä½“æ¥æºäºçš„æ‰€æœ‰æ–‡æ¡£å—ï¼ˆchunksï¼‰ï¼Œå¹¶è‡ªåŠ¨å»é‡å’Œè¿‡æ»¤ã€‚æ”¯æŒè·å–å®Œæ•´çš„ chunk å†…å®¹ã€‚

è¾“å…¥å‚æ•°:
- entity_id: string (å¿…éœ€) - å®ä½“çš„ç²¾ç¡® IDï¼Œå¿…é¡»åŒ…å«éšæœºåç¼€ï¼Œä¾‹å¦‚ï¼š"çº¿æ®µæ ‘<QyCKb7>"ã€"åŠ¨æ€ç»´æŠ¤<x7YLqt>"
- include_content: boolean (å¯é€‰) - æ˜¯å¦åŒ…å« chunk çš„å®Œæ•´å†…å®¹ï¼Œé»˜è®¤ true

è¿”å›æ ¼å¼:
JSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
{
  "chunk_id": "chunk-<hash>",  // chunk çš„å”¯ä¸€æ ‡è¯†ç¬¦
  "status": "available"        // chunk çŠ¶æ€
}

å¦‚æœ include_content=trueï¼Œè¿”å›:
{
  "chunk_id": "chunk-<hash>",
  "content": "chunk å®Œæ•´æ–‡æœ¬å†…å®¹",
  "tokens": 123,
  "file_path": "æºæ–‡ä»¶è·¯å¾„",
  "full_doc_id": "æ–‡æ¡£ID",
  "chunk_order_index": 0,
  "source_id": "æºID",
  "status": "available" | "not_found"
}

ä½¿ç”¨ç¤ºä¾‹:
1. è·å–å®ä½“å…³è”çš„ chunk ID åˆ—è¡¨: {"entity_id": "çº¿æ®µæ ‘<QyCKb7>"}
2. è·å–åŒ…å«å†…å®¹çš„ chunk åˆ—è¡¨: {"entity_id": "çº¿æ®µæ ‘<QyCKb7>", "include_content": true}

æ³¨æ„äº‹é¡¹:
- entity_id å¿…é¡»æ˜¯å®Œæ•´çš„å®ä½“ IDï¼ŒåŒ…å«å°–æ‹¬å·å’Œéšæœºåç¼€
- å¦‚æœä¸çŸ¥é“å®Œæ•´ IDï¼Œå¯ä»¥å…ˆä½¿ç”¨ find_similar_entities æŸ¥æ‰¾å®ä½“
- chunk åˆ—è¡¨ä¼šè‡ªåŠ¨å»é‡å’Œè¿‡æ»¤é chunk å†…å®¹
- æœ€å¤šè¿”å› 20 ä¸ª chunksï¼Œè¶…å‡ºéƒ¨åˆ†ä¼šè¢«æˆªæ–­
"""
)
async def get_chunks(
    entity_id: str,
    include_content: bool = True,
) -> List[Dict[str, Any]]:
    """è·å–å®ä½“å…³è”çš„ chunks"""
    # ç›´æ¥è°ƒç”¨ï¼Œé¿å… asyncio.wait_for ä¸ FastMCP äº‹ä»¶å¾ªç¯å†²çª
    results = await service.get_entity_chunks(
        entity_id=entity_id,
        include_content=include_content,
    )
    # è¿”å›åŸå§‹æ•°æ®ï¼Œè®©FastMCPè‡ªåŠ¨å¤„ç†åºåˆ—åŒ–
    return results

def create_embedding_func():
    """åˆ›å»ºåµŒå…¥å‡½æ•°"""
    from lightrag.llm.openai import openai_embed
    from lightrag.utils import EmbeddingFunc

    embedding_dim = int(os.getenv("EMBEDDING_DIM", "1024"))
    embedding_model = os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")
    embedding_host = os.getenv("EMBEDDING_BINDING_HOST", "https://api.siliconflow.cn/v1")
    embedding_api_key = os.getenv("EMBEDDING_BINDING_API_KEY", "")

    logger.info(f"[INFO] Embedding configuration:")
    logger.info(f"  Dimension: {embedding_dim}")
    logger.info(f"  Model: {embedding_model}")
    logger.info(f"  Service: {embedding_host}")

    async def embedding_call(texts):
        """å¼‚æ­¥ embedding è°ƒç”¨"""
        return await openai_embed(
            texts,
            model=embedding_model,
            base_url=embedding_host,
            api_key=embedding_api_key,
        )

    embedding_func = EmbeddingFunc(
        embedding_dim=embedding_dim,
        max_token_size=8192,
        func=embedding_call,
    )
    return embedding_func


if __name__ == "__main__":
    # åˆ›å»º embedding å‡½æ•°
    embedding_func = create_embedding_func()

    # é¢„åˆå§‹åŒ–æœåŠ¡ï¼ˆå¸¦ embedding æ”¯æŒï¼‰
    async def init_service():
        # åˆå§‹åŒ– Neo4j æœåŠ¡
        await service.initialize(embedding_func=embedding_func)
        # # åˆå§‹åŒ– LightRAG ç®¡ç†å™¨ï¼ˆç”¨äºè·å– chunk å†…å®¹ï¼‰
        # await lightrag_manager.initialize()

    asyncio.run(init_service())
    import asyncio
    # ä»ç¯å¢ƒå˜é‡è¯»å–MCPé…ç½®
    mcp_host = os.getenv("MCP_HOST", "127.0.0.1")
    mcp_port = int(os.getenv("MCP_PORT", "8000"))
    # ä»ç¯å¢ƒå˜é‡è¯»å–æ—¥å¿—ç­‰çº§ï¼ˆé»˜è®¤ INFOï¼‰ï¼Œå¹¶è½¬æ¢ä¸ºå°å†™
    mcp_log_level = os.getenv("LOG_LEVEL", "INFO").lower()
    asyncio.run(mcp.run_async(
            transport="streamable-http",
            host=mcp_host,
            port=mcp_port,
            log_level=mcp_log_level,
            stateless_http=True
        ))
