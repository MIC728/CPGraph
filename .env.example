### This is sample file of .env

### Logging level
LOG_LEVEL=INFO

########################################
### Document processing configuration
########################################
ENABLE_LLM_CACHE_FOR_EXTRACT=true

### Document processing output language: English, Chinese, French, German ...
SUMMARY_LANGUAGE=Chinese

### Entity types that the LLM will attempt to recognize
### Two-dimensional entity types for enhanced classification
# First dimension (technical classification)
ENTITY_TYPES_DIM1='["数据结构", "算法思想", "动态规划", "图论", "搜索", "字符串", "数学", "计算几何"]'
# Second dimension (application level)
ENTITY_TYPES_DIM2='["概念", "技巧", "实现", "模型", "算法", "原理", "题目"]'

### Chunk size for document splitting, 500~1500 is recommended
CHUNK_SIZE=1200
CHUNK_OVERLAP_SIZE=100

###############################
### Concurrency Configuration
###############################
### Max concurrency requests of LLM (for both query and document processing)
MAX_ASYNC=24
### Number of parallel processing documents(between 2~10, MAX_ASYNC/3 is recommended)
MAX_PARALLEL_INSERT=8
### Max concurrency requests for Embedding
EMBEDDING_FUNC_MAX_ASYNC=8
### Num of chunks send to Embedding in single request
EMBEDDING_BATCH_NUM=10
EMBEDDING_MAX_CONCURRENT=16

###########################################################################
### LLM Configuration (OpenAI Compatible Only)
###########################################################################
LLM_TIMEOUT=600

LLM_BINDING=openai
LLM_MODEL=deepseek-chat
LLM_BINDING_HOST=https://api.deepseek.com/v1
LLM_BINDING_API_KEY=your_llm_api_key_here

### OpenAI Compatible API Parameters
OPENAI_LLM_MAX_COMPLETION_TOKENS=9000

#######################################################################################
### Embedding Configuration (OpenAI Compatible Only)
#######################################################################################
EMBEDDING_BINDING=openai
EMBEDDING_MODEL=Qwen/Qwen3-Embedding-4B
EMBEDDING_DIM=2560
EMBEDDING_BINDING_API_KEY=your_embedding_api_key_here
EMBEDDING_BINDING_HOST=https://api.siliconflow.cn/v1

### Neo4j Configuration
NEO4J_URI=bolt://127.0.0.1:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your_neo4j_password
NEO4J_DATABASE=neo4j
NEO4J_MAX_CONNECTION_POOL_SIZE=50
NEO4J_CONNECTION_TIMEOUT=120
NEO4J_CONNECTION_ACQUISITION_TIMEOUT=120
NEO4J_MAX_TRANSACTION_RETRY_TIME=120
NEO4J_MAX_CONNECTION_LIFETIME=600
NEO4J_LIVENESS_CHECK_TIMEOUT=60
NEO4J_KEEP_ALIVE=true

### MCP Server Configuration
### MCP server listening port (default: 8000)
MCP_PORT=8000
### MCP server listening host (default: 127.0.0.1)
MCP_HOST=127.0.0.1

### Neo4j Connection Pool Configuration
### Number of concurrent Neo4j connections for MCP server (default: 4)
NEO4J_POOL_SIZE=4

###########################
### Performance Profiling
###########################
### Enable timing for performance analysis
LIGHTRAG_ENABLE_TIMING=true
LIGHTRAG_TIMING_MIN_DURATION=0.1
LIGHTRAG_TIMING_MAX_DEPTH=10

#######################################################################################
### LLM Usage Tracking Configuration
### Enable LLM usage tracking for monitoring API calls, token usage, and costs
#######################################################################################
ENABLE_LLM_TRACKING=true

#######################################################################################
### Entity Merge Configuration (entity_merge.py)
### Entity Merge Module Configuration
#######################################################################################
### Incremental update switch: true=incremental update (read existing data then merge), false=full overwrite
ENTITY_MERGE_INCREMENTAL=true
### Clear Neo4j database before saving (default: true)
CLEAR_NEO4J=true
### Data directory path configuration
### file_extract.py output directory (default: ./extracted_data)
EXTRACTOR_OUTPUT_DIR=./extracted_data
### entity_merge.py output directory (default: ./merged_data)
MERGED_DATA_DIR=./merged_data
